---
title: "Homework #6: Linear Models"
author: "Pooja Desai (pmd2137)"
date: "12/3/2022"
output: github_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
library(dplyr)
library(tidyverse)
library(ggplot2)
library(purrr)
library(modelr)
library(viridis)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Problem 1

For this problem, we'll use the 2017 Central Park weather data that we've seen elsewhere. The code chunk below (adapted from the course website) will download these data.

```{r weather data}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

The boostrap is helpful when you'd like to perform inference for a parameter / value / summary that doesn't have an easy-to-write-down distribution in the usual repeated sampling framework. We'll focus on a simple linear regression with tmax as the response and tmin as the predictor, and are interested in the distribution of two quantities estimated from these data:

-   r̂ 2
-   log(β̂ 0∗β̂ 1)

Use 5000 bootstrap samples and, for each bootstrap sample, produce estimates of these two quantities.

Plot the distribution of your estimates, and describe these in words.

Using the 5000 bootstrap estimates, identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval for r̂ 2 and log(β̂ 0∗β̂ 1).

Note: broom::glance() is helpful for extracting r̂ 2 from a fitted regression, and broom::tidy() (with some additional wrangling) should help in computing log(β̂ 0∗β̂ 1).

```{r weather solution}
#Part 1: Plot distribution for R^2 (course provided code)
weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()

## Part 2: Plot distribution for log beta (course provided code)
weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()

```

## Problem 2: Homidcide Dataset

The Washington Post has gathered data on homicides in 50 large U.S. cities and made the data available through GitHub. We completed the following steps to preprocess the data, similar to previous assignments using this dataset:

First, we created some new variables:

-   `city_state` is variable that joins the city and state fields (separated by a comma).

-   `solved` is a binary variable that denotes whether or not a case was solved.

Second, we update the variables to convert `race` and `sex` are factors. We also convert `age` to a numeric variable. Third, we filter on the `race` variable to limit the analysis to victims whose race is identified as Black or White.

Finally, we removed the following `city_state` values because they were entry errors or did not report victim race: Dallas, TX; Phoenix, AZ; Kansas City, MO; Tulsa, AL.

```{r echo=TRUE}
homicides_df <- read.csv("data/homicide-data.csv")  %>%
  janitor::clean_names() %>%
  mutate(city_state = paste(city,state,sep = ", "),
         solved = ifelse(disposition=="Closed by arrest", 1, 0)) %>%
  
  #convert variables to factors and numeric
  mutate(disposition = as.factor(disposition), 
         victim_race = as.factor(victim_race),
         victim_sex = as.factor(victim_sex),
         victim_age = as.numeric(victim_age)) %>%
  
    #filter by victim race
  filter(victim_race %in% c("Black", "White")) %>%

  #remove erroneous city state
  filter(city_state != "Tulsa, AL", 
         city_state != "Dallas, TX", 
         city_state != "Phoenix, AZ", 
         city_state != "Kansas City, MO") -> cleaned_dataset
```

For the city of Baltimore, MD we used the glm function to fit a logistic regression with case `solved` as the outcome and `victim age`, `victim_sex` and `victim_race` as predictors. 

From we output we obtained estimates, as use them to calculate the adjusted odds ratio and 95% confidence interval for each of the predictors. The results are in the table below. 

One question of interest was the adjusted odds ratio for solving homicides comparing *male victims* to *female victims* keeping all other variables fixed. 

Keeping all other varibales constant, the odds of solving cases with male victims is 0.60. This means male victims have a reduction of 40% in the odds of their homicide being solved compared to females. 

```{r}
#construct glm model
homicide_logistic = 
  cleaned_dataset %>%
  glm(solved ~ victim_age + victim_sex + victim_race, data=., family = binomial())

#obtain estimated from model
homicide_logistic %>%
    broom::tidy() %>% 
    
  #calculate odds
    mutate(OR = exp(estimate), #convert to odds
           CI.lower.95 = exp(estimate-(1.96*std.error)), #lower CI @ 95%
           CI.upper.95 = exp(estimate+(1.96*std.error))) %>% # upper CI @ 95%
    select(term, log_OR = estimate, OR, CI.lower.95, CI.upper.95, p.value) %>%
    filter(term == "victim_sexMale")
```
Them we ran glm for each of the cities in the dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims. 

```{r}
#created function to calculate odds ratio per city
odds_male_solved = function(city) {
subset = cleaned_dataset$city_state==city

#create glm
homicide_logistic = 
  cleaned_dataset[subset,] %>%
  glm(solved ~ victim_age + victim_sex + victim_race, data=., family = binomial())

#obtain estimated from model
homicide_logistic %>%
    broom::tidy() %>% 
    
  #calculate odds
    mutate(OR = exp(estimate), #convert to odds
           CI.lower.95 = exp(estimate-(1.96*std.error)), #lower CI @ 95%
           CI.upper.95 = exp(estimate+(1.96*std.error))) %>% # upper CI @ 95%
  filter(term == "victim_sexMale") %>%
  mutate(city_state = city) %>%
  select(city_state, log_OR = estimate, OR, CI.lower.95, CI.upper.95, p.value) -> values

return(values)}

#test function on example city
odds_male_solved("Baltimore, MD")
```

Apply function above to all of the cities and states in the dataframe.

```{r}
# extract unique list of cities 
city_list <- unique(homicides_df$city_state)

# For cities in dataset, extract odds case solved male
city_odds_male_solved <- purrr::map_df(city_list, odds_male_solved)
```

The graph below shows the odds of solving a homicide case for males compared to females, keeping all other demographic variables (race, age) constant in each city.

```{r pressure, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

# load the library
library(forcats)

#reordered dataset by estimate (descending)
city_odds_male_solved %>%
  mutate(name = fct_reorder(city_state, OR)) %>%
  ggplot(aes(x = name, y = OR)) + 
    geom_point() + #add estimates
    geom_errorbar(aes(ymin=CI.lower.95, ymax = CI.upper.95)) + #add error bars
    coord_flip() +
    xlab("Odds of Solved Homicide for Male") +
    ylab("City, State")

```
From this graph, we can see that the odds of solving a homicide where the victim is male is less than 1 in most city, states across the United States. This means that the odds of solving a homicide case where the victim is male case is lower than than the odds of solving a homicide case where the victim is female (all else being equal) in most Cities with homicides. Some notable exceptions are Nashville, TN; Fresno, CA; Stockton, CA; and Albuquerque, NM where the odds of solving a homicide if the vicitim is female is lower than if the victim is male.

## Problem 3:

In this problem, you will analyze data gathered to understand the effects of several variables on a child's birthweight. This dataset, available here, consists of roughly 4000 children and includes the following variables:

```{r}
birthweight <- read.csv("data/birthweight.csv") 
```

        babysex: baby’s sex (male = 1, female = 2)
        bhead: baby’s head circumference at birth (centimeters)
        blength: baby’s length at birth (centimeteres)
        bwt: baby’s birth weight (grams)
        delwt: mother’s weight at delivery (pounds)
        fincome: family monthly income (in hundreds, rounded)
        frace: father’s race (1 = White, 2 = Black, 3 = Asian, 4 = Puerto Rican, 8 = Other, 9 = Unknown)
        gaweeks: gestational age in weeks
        malform: presence of malformations that could affect weight (0 = absent, 1 = present)
        menarche: mother’s age at menarche (years)
        mheigth: mother’s height (inches)
        momage: mother’s age at delivery (years)
        mrace: mother’s race (1 = White, 2 = Black, 3 = Asian, 4 = Puerto Rican, 8 = Other)
        parity: number of live births prior to this pregnancy
        pnumlbw: previous number of low birth weight babies
        pnumgsa: number of prior small for gestational age babies
        ppbmi: mother’s pre-pregnancy BMI
        ppwt: mother’s pre-pregnancy weight (pounds)
        smoken: average number of cigarettes smoked per day during pregnancy
        wtgain: mother’s weight gain during pregnancy (pounds)
        Load and clean the data for regression analysis (i.e. convert numeric to factor where appropriate, check for missing data, etc.).

Propose a regression model for birthweight. This model may be based on a hypothesized structure for the factors that underly birthweight, on a data-driven model-building process, or a combination of the two. Describe your modeling process and show a plot of model residuals against fitted values -- use add_predictions and add_residuals in making this plot.

Compare your model to two others:

One using length at birth and gestational age as predictors (main effects only) One using head circumference, length, sex, and all interactions (including the three-way interaction) between these Make this comparison in terms of the cross-validated prediction error; use crossv_mc and functions in purrr as appropriate.

Note that although we expect your model to be reasonable, model building itself is not a main idea of the course and we don't necessarily expect your model to be "optimal".
